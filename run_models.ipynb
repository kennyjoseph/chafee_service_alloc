{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001-09-30\n",
      "21457\n",
      "350\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from functools import partial\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# show all the columns when printing\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "ANCHOR_DATE = date(2001,9,30) - relativedelta(days=15248)\n",
    "\n",
    "def convert_int_date_to_real_date(int_date):\n",
    "    return ANCHOR_DATE + relativedelta(days=int_date)\n",
    "\n",
    "def convert_real_date_to_int_date(real_date):\n",
    "    return (real_date-ANCHOR_DATE).days\n",
    "\n",
    "# Test Date Crap\n",
    "fy_end = date(2018,9,30)\n",
    "print(convert_int_date_to_real_date(15248))\n",
    "print(convert_real_date_to_int_date(fy_end))\n",
    "print(convert_real_date_to_int_date(fy_end)-convert_real_date_to_int_date(fy_end-relativedelta(days=350)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_cols = [\"SpecEdSv\",\n",
    "\"ILNAsv\",\n",
    "\"AcSuppSv\",\n",
    "\"PSEdSuppSv\",\n",
    "\"CareerSv\",\n",
    "\"EmplyTrSv\",\n",
    "\"BudgetSv\",\n",
    "\"HousEdSv\",\n",
    "\"HlthEdSv\",\n",
    "\"FamSuppSv\",\n",
    "\"MentorSv\",\n",
    "\"SILsv\",\n",
    "\"RmBrdFASv\",\n",
    "\"EducFinaSv\",\n",
    "\"OthrFinaSv\"]\n",
    "\n",
    "# No special ed for prediction, very distinct from the other services theoretically\n",
    "all_service_cols = [x for x in services_cols if x !=\"SpecEdSv\"]\n",
    "\n",
    "potential_outcomes = all_service_cols+[\"AllServices_Binary\",\n",
    "#\"AllServices_Full\",\n",
    "\"FinancialServices_Binary\",\n",
    "#\"FinancialServices_Full\",\n",
    "\"YoungerServices_Binary\"]#,\n",
    "#\"YoungerServices_Full\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2018\"\n",
    "\n",
    "fy_start = date(int(YEAR)-1, 10,1)\n",
    "fy_end = date(int(YEAR),9,30)\n",
    "fy_end_date_int = (fy_end -ANCHOR_DATE).days\n",
    "fy_start_date_int = (fy_start-ANCHOR_DATE).days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Services data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "services_data = pd.read_sas(\"/data/afcars/nytd/services/Data/SAS/services2018_f.sas7bdat\")\n",
    "#treat 77s and nulls as clerical errors / incomplete data\n",
    "services_data = services_data[~(services_data[all_service_cols].isnull()).any(axis=1)]\n",
    "services_data[~(services_data[all_service_cols] == 77).any(axis=1)]\n",
    "services_data = services_data[['StFCID','FY'] + services_cols]\n",
    "\n",
    "service_counts = services_data.groupby(['FY','StFCID']).sum().reset_index()\n",
    "service_counts[\"AllServices_Binary\"] = (service_counts[all_service_cols] > 0).sum(axis=1)\n",
    "\n",
    "s_cols = [\"SILsv\",\"RmBrdFASv\",\"EducFinaSv\",\"OthrFinaSv\"]\n",
    "service_counts[\"FinancialServices_Binary\"] = (service_counts[s_cols] > 0).sum(axis=1)\n",
    "\n",
    "s_cols = [\"AcSuppSv\", \"PSEdSuppSv\", \"CareerSv\", \"EmplyTrSv\", \"BudgetSv\", \n",
    "          \"HousEdSv\", \"HlthEdSv\", \"FamSuppSv\", \"MentorSv\"]\n",
    "service_counts[\"YoungerServices_Binary\"] = (service_counts[s_cols] > 0).sum(axis=1)\n",
    "\n",
    "service_counts.to_csv(\"services_counts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AFCARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py:800: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rslt[name] = self._byte_chunk[jb, :].view(dtype=self.byte_order + \"d\")\n",
      "/opt/anaconda/lib/python3.7/site-packages/pandas/io/sas/sas7bdat.py:809: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  rslt[name] = self._string_chunk[js, :]\n"
     ]
    }
   ],
   "source": [
    "afcars_prevyear = None\n",
    "afcars_curryear = None\n",
    "if YEAR == \"2018\":\n",
    "    afcars_prevyear = pd.read_sas('/data/afcars/all/AFCARS Foster Care/2017/Data/SAS Files/fc17v2f.sas7bdat')\n",
    "    afcars_curryear = pd.read_sas('/data/afcars/all/AFCARS Foster Care/2018/DATA/SAS Files/fc2018v1.sas7bdat') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((691188, 105), (687402, 105))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afcars_prevyear.shape, afcars_curryear.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_counts['StFCID'] = service_counts.StFCID.astype(\"string\")\n",
    "afcars_curryear['StFCID'] = afcars_curryear.StFCID.astype(\"string\")\n",
    "afcars_merged_with_services = pd.merge(afcars_curryear,\n",
    "                                       service_counts[service_counts.FY == int(YEAR)],\n",
    "                                       on=[\"StFCID\"],\n",
    "                                       how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/ipykernel_launcher.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider using pd.concat instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "afcars_merged_with_services['in_afcars_prevyear'] = afcars_merged_with_services.StFCID.isin(afcars_prevyear.StFCID.astype(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = afcars_merged_with_services.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[potential_outcomes] = df[potential_outcomes].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tack on previous year services\n",
    "prev_year_services = service_counts[service_counts.FY == (int(YEAR)-1)].copy().drop(columns=['FY',\"SpecEdSv\"])\n",
    "prev_year_services.columns = ['StFCID'] + ['prev_'+c for c in potential_outcomes]\n",
    "df = df.merge(prev_year_services, on='StFCID',how='left')\n",
    "cols_to_fill = [x for x in prev_year_services.columns if x !=\"StFCID\"]\n",
    "df[cols_to_fill] = df[cols_to_fill].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By inclusion criterion, have all of these\n",
    "df['first_removal_date_int']  = fy_end_date_int - df.Rem1Dt\n",
    "df['latest_removal_date_int'] = fy_end_date_int - df.LatRemDt\n",
    "df['latest_setting_date_int']  = fy_end_date_int - df.CurSetDt\n",
    "df['age_at_end_int'] = fy_end_date_int - df.DOB\n",
    "\n",
    "# These might be null\n",
    "df['previous_removal_discharge_date_int']  = fy_end_date_int - df.DLstFCDt\n",
    "df.loc[df.DLstFCDt.isnull(),'previous_removal_discharge_date_int'] = 0\n",
    "\n",
    "df['latest_removal_discharge_date_int']  = fy_end_date_int - df.DoDFCDt\n",
    "df.loc[df.DoDFCDt.isnull(),'latest_removal_discharge_date_int'] = 0\n",
    "\n",
    "df['periodic_review_date_int']  = fy_end_date_int - df.PedRevDt\n",
    "df.loc[df.PedRevDt.isnull(),'periodic_review_date_int'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1801, 149)\n",
      "(2287, 149)\n",
      "(8261, 149)\n",
      "(243, 149)\n",
      "(3496, 149)\n",
      "(16545, 149)\n",
      "(2, 149)\n"
     ]
    }
   ],
   "source": [
    "# oddities of negative values, seems reasonable though that it's updated to post-reporting period?\n",
    "print(df[df.first_removal_date_int < 0].shape)\n",
    "print(df[df.latest_removal_date_int < 0].shape)\n",
    "print(df[df.latest_setting_date_int < 0].shape)\n",
    "print(df[df.previous_removal_discharge_date_int < 0].shape)\n",
    "print(df[df.latest_removal_discharge_date_int < 0].shape)\n",
    "print(df[df.periodic_review_date_int < 0].shape)\n",
    "print(df[df.age_at_end_int < 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    55408\n",
       "0.0    54577\n",
       "2.0    49690\n",
       "3.0    43994\n",
       "4.0    39562\n",
       "Name: AgeAtStart, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.AgeAtStart.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows we don't want to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df,inclusion_criterion):\n",
    "    pre = len(df)\n",
    "    df = df[inclusion_criterion]\n",
    "    print(\"N removed: {}, % Removed: {}\".format(pre-len(df), (pre-len(df))/pre))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_data_length = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 30, % Removed: 4.3642584688435587e-05\n",
      "N removed: 552641, % Removed: 0.8039911430782749\n",
      "N removed: 49189, % Removed: 0.3650904394682738\n"
     ]
    }
   ],
   "source": [
    "# Drop people who are older than 22 (few)\n",
    "df = drop_rows(df, (df.AgeAtStart < 22) & (df.age_at_end_int <= 23*365.25))\n",
    "\n",
    "# Drop people who are younger than 14 (limited services)\n",
    "df = drop_rows(df, (df.AgeAtStart > 13) & (df.age_at_end_int >= 14*365.25))\n",
    "\n",
    "\n",
    "# Drop people who were in case for > 6 months of 2018\n",
    "# These variables don't always agree (errors?), so just checking all of them\n",
    "df = drop_rows(df, (df.latest_removal_discharge_date_int <= 180) & \n",
    "                   (df.LatRemLOS >= 180) &\n",
    "                   (df.latest_removal_date_int >= 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 13, % Removed: 0.0001519721306492717\n"
     ]
    }
   ],
   "source": [
    "# Drop individuals with first removal date 22+ years ago (couldn't be born)\n",
    "df = drop_rows(df, df.first_removal_date_int <= (fy_start - (fy_start- relativedelta(years=22))).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 85, % Removed: 0.0009938149633457658\n"
     ]
    }
   ],
   "source": [
    "# Drop individuals we don't have latest removal date\n",
    "# and/or first removal date \n",
    "# and/or latest setting change date for. \n",
    "df = drop_rows(df, (df.LatRemDt != 0) & (df.Rem1Dt != 0) & (df.CurSetDt) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 0, % Removed: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Drop Youth whose Sex is unknown, we consider this a parameter of interest\n",
    "df = drop_rows(df, df.Sex != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 883, % Removed: 0.010334254014325172\n"
     ]
    }
   ],
   "source": [
    "# Drop Youth whose Race/Ethnicity is unknown, we consider this a parameter of interest\n",
    "df = drop_rows(df, df.RaceEthn != 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 2172, % Removed: 0.025685599744563095\n"
     ]
    }
   ],
   "source": [
    "# Drop states with no service data at all\n",
    "sm = df.groupby(\"St\").AllServices_Binary.mean()\n",
    "df = drop_rows(df, ~df.St.isin(sm[sm == 0].index.values.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 931, % Removed: 0.011300052191433322\n"
     ]
    }
   ],
   "source": [
    "# Drop people who don't have a setting LOS, seems important\n",
    "df = drop_rows(df, ~df.SettingLOS.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N removed: 710, % Removed: 0.008716148199071915\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with PrtsDied is null. These seemed to have a lot of nulls, too many to safely impute\n",
    "df = drop_rows(df, ~df.PrtsDied.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total removed: 606654, total percent removed: 0.8825316190526068'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total removed: {}, total percent removed: {}\".format(initial_data_length - len(df),\n",
    "                                                     (initial_data_length-len(df))/initial_data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total youth remaining: 80748'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total youth remaining: {}\".format(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age(x):\n",
    "    if x < 19:\n",
    "        return str(x).replace(\".0\",\"\")\n",
    "    return \"19+\"\n",
    "# Convert age to discrete, no reason to expect continuous is useful\n",
    "df['age'] = df.AgeAtStart.apply(get_age).astype(\"string\")\n",
    "# Clean up state\n",
    "df['St'] = df.St.astype(\"string\")\n",
    "df['St'] = df.St.str.replace(\"b'\",\"\").str.replace(\"'\",\"\")\n",
    "# Create var for age/state/year\n",
    "df['age_state_year'] = df.St+ \"_\" + df.age + \"_\" + YEAR\n",
    "# Make Sex Binary\n",
    "df['Sex'] = df.Sex -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for binary checks\n",
    "\n",
    "\n",
    "df['dad_rights_term'] = ~df.TPRDadDt.isnull() * 1\n",
    "df['mom_rights_term'] = ~df.TPRMomDt.isnull() * 1\n",
    "df['had_periodic_review'] = df.periodic_review_date_int.apply(lambda x: 0 if x == 0 else 1)\n",
    "df['was_discharged'] = df.latest_removal_discharge_date_int.apply(lambda x: 0 if x == 0 else 1)\n",
    "df['previous_was_discharged'] = df.previous_removal_discharge_date_int.apply(lambda x: 0 if x == 0 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RU13_cont'] = df['RU13'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all features, add log of continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "continuous_features = [ \n",
    "    \"SettingLOS\",\n",
    "    \"TotalRem\",\n",
    "    \"NumPlep\",\n",
    "    'first_removal_date_int',\n",
    "    'latest_removal_date_int',\n",
    "    'latest_setting_date_int',\n",
    "    'previous_removal_discharge_date_int',\n",
    "    'latest_removal_discharge_date_int',\n",
    "    'periodic_review_date_int',\n",
    "    'age_at_end_int',\n",
    "    \"RU13\"\n",
    "] \n",
    "\n",
    "for f in continuous_features:\n",
    "    if f ==\"RU13\":\n",
    "        continue\n",
    "    if(df[f].min() < 0):\n",
    "        df[\"log_\"+f]=np.log(df[f]+1-df[f].min())\n",
    "    else:\n",
    "        df[\"log_\"+f]=np.log(df[f]+1)\n",
    "log_continuous_features = [\"log_\"+f for f in continuous_features if f !=\"RU13\"]\n",
    "\n",
    "previous_year_service_features = [\"prev_\"+x for x in potential_outcomes]\n",
    "\n",
    "binary_features = [\n",
    "    \"had_periodic_review\",\n",
    "    \"was_discharged\",\n",
    "    \"previous_was_discharged\",\n",
    "    \"dad_rights_term\", \n",
    "    \"mom_rights_term\",\"AgedOut\",\n",
    "    \"SexAbuse\", \"PhyAbuse\", \"Neglect\", \n",
    "    \"AAParent\", \"DAParent\", \"AAChild\", \"DAChild\", \"ChilDis\", \n",
    "    \"ChBehPrb\", \"PrtsDied\", \"PrtsJail\", \"NoCope\", \"Abandmnt\",\n",
    "    \"Relinqsh\", \"Housing\", \"IsTPR\", \"IVEFC\", \"IVEAA\",\n",
    "    \"IVAAFDC\", \"IVDCHSUP\", \"XIXMEDCD\", \"SSIOther\", \"NOA\",\n",
    "    \"MR\", \"VisHear\", \"PhyDis\", \"EmotDist\", \n",
    "    \"OtherMed\",\n",
    "    \"Sex\",\n",
    "    \"in_afcars_prevyear\"\n",
    "]\n",
    "\n",
    "discrete_features = [\n",
    "    #\"RU13\",\n",
    "    \"RaceEthn\", \"CaseGoal\", \"CurPlSet\",\n",
    "    \"ClinDis\",  \"ManRem\", \n",
    "     \"CtkFamSt\",   \"PlaceOut\",\"St\",\n",
    "    \"age\",\n",
    "    \"AgeAdopt\",\"FosFamSt\",\"DISREASN\",\"IsWaiting\"\n",
    "]\n",
    "\n",
    "\n",
    "manual_features = [\n",
    "    \"NumPlep\",\n",
    "    'log_first_removal_date_int',\n",
    "    'log_latest_removal_date_int',\n",
    "    'log_latest_setting_date_int',\n",
    "    'log_previous_removal_discharge_date_int',\n",
    "    'log_latest_removal_discharge_date_int',\n",
    "    \"had_periodic_review\",\n",
    "    \"was_discharged\",\n",
    "    \"AgedOut\",\n",
    "    \"in_afcars_prevyear\",\n",
    "    \"RU13\",\n",
    "    \"RaceEthn\", \"CaseGoal\", \"CurPlSet\",\"St\",\"age\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "all_vars = continuous_features + log_continuous_features + previous_year_service_features + binary_features+discrete_features + potential_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our package\n",
    "from pandas_profiling import ProfileReport\n",
    "from pandas_profiling.utils.cache import cache_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_df = df.copy()\n",
    "for var_name in discrete_features + binary_features:\n",
    "    vis_df[var_name] = vis_df[var_name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Profiling Report\n",
    "profile = ProfileReport(\n",
    "    vis_df[all_vars], title=\"Services Pred: \"+ YEAR, \n",
    "    html={\"style\": {\"full_width\": True}}, \n",
    "    sort=None,\n",
    "    minimal=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Notebook Widgets Interface\n",
    "profile.to_file(\"output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Imputation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA shows some missing values. For now, we will impute basic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalRem has 42 nulls, replacing with 1s\n",
      "NumPlep has 128 nulls, replacing with 1s\n",
      "RU13 has 3 nulls, replacing with 0s\n",
      "IVDCHSUP has 2 nulls, replacing with 0s\n",
      "XIXMEDCD has 27 nulls, replacing with 0s\n",
      "SSIOther has 21 nulls, replacing with 0s\n",
      "MR has 1494 nulls, replacing with 0s\n",
      "VisHear has 1500 nulls, replacing with 0s\n",
      "PhyDis has 1493 nulls, replacing with 0s\n",
      "EmotDist has 1493 nulls, replacing with 0s\n",
      "OtherMed has 1493 nulls, replacing with 0s\n",
      "Sex has 4 nulls, replacing with 0s\n",
      "ClinDis has 2381 nulls, replacing with 3s\n",
      "ManRem has 24 nulls, replacing with 0s\n",
      "CtkFamSt has 1952 nulls, replacing with 5s\n",
      "PlaceOut has 150 nulls, replacing with 0s\n",
      "AgeAdopt has 4954 nulls, replacing with 5s\n",
      "FosFamSt has 5717 nulls, replacing with 0s\n",
      "log_TotalRem has 42 nulls, replacing with 0s\n",
      "log_NumPlep has 128 nulls, replacing with 0s\n"
     ]
    }
   ],
   "source": [
    "base_values = {\"AgeAdopt\" : 5, \"CtkFamSt\" : 5, \"ClinDis\": 3, \"TotalRem\" : 1, \"NumPlep\": 1,\n",
    "              \"latest_removal_date_int\" : 180}\n",
    "for col in continuous_features+binary_features+discrete_features+log_continuous_features:\n",
    "    null_count  = df[col].isnull().sum()\n",
    "    if null_count > 0:\n",
    "        print(\"{} has {} nulls, replacing with {}s\".format(col,null_count,base_values.get(col,0)))\n",
    "    df[col] = df[col].fillna(base_values.get(col,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_regression_features = {\"prev_AllServices_Binary\",\n",
    "                         \"prev_YoungerServices_Binary\",\n",
    "                         \"prev_FinancialServices_Binary\"}\n",
    "def create_r_formula_string(features,outcome, factor_phrase):\n",
    "    base_reg_form = \"+\".join([factor_phrase+\"(\"+x+\")\" for x in features if (x in discrete_features and\n",
    "                                                                           x != \"St\" and\n",
    "                                                                           x != \"age\")])\n",
    "    if factor_phrase != \"i\":\n",
    "        if 'St' in features:\n",
    "            base_reg_form += \" + factor(St)\"\n",
    "        if 'age' in features:\n",
    "            base_reg_form += \" + factor(age)\"\n",
    "            \n",
    "    base_reg_form += \"+\" + \"+\".join([x for x in features if x not in discrete_features\n",
    "                                     and x not in no_regression_features])\n",
    "    base_reg_form = outcome + \" ~ \" + base_reg_form\n",
    "    if factor_phrase == \"i\":\n",
    "        base_reg_form += \" | age^St\"\n",
    "    return base_reg_form\n",
    "\n",
    "r_zeroinfl_regression_call = ro.r('''\n",
    "function(reg_form,train_index,test_index){\n",
    "    library(glmmTMB)\n",
    "    fit_zipoisson <- glmmTMB(formula(reg_form),\n",
    "                        data=regression_data[train_index+1,],\n",
    "                        ziformula=~St+age+factor(CaseGoal)*had_periodic_review+factor(CurPlSet)+\n",
    "                        factor(DISREASN)+was_discharged,\n",
    "                        family=nbinom2)\n",
    "    return(predict(fit_zipoisson,newdata=regression_data[test_index+1,], type=\"response\",na.rm=F))\n",
    "}\n",
    "    \n",
    "''')\n",
    "\n",
    "r_negbin_regression_call = ro.r('''\n",
    "function(reg_form,train_index,test_index){\n",
    "    library(fixest)\n",
    "    f <- femlm(formula(reg_form),\n",
    "       family=\"negbin\",\n",
    "        data=regression_data[train_index+1,],\n",
    "        se=\"cluster\",\n",
    "        combine.quick=FALSE)\n",
    "    return(predict(f,newdata=regression_data[test_index+1,], na.rm=F))\n",
    "}\n",
    "    \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def basic_groupby_model(groupby_vars,\n",
    "                        data,\n",
    "                        outcome_var,\n",
    "                        train_index,\n",
    "                        test_index):\n",
    "    predvar = 'pred_base'+\"_\".join(groupby_vars)\n",
    "    pred = data.iloc[train_index].groupby(groupby_vars)[outcome_var].mean().reset_index()\n",
    "    pred.columns = groupby_vars+ [predvar]\n",
    "    return pd.merge(data.iloc[test_index],pred,on=groupby_vars, how=\"left\")[predvar]\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=1000,\n",
    "                                      max_features='auto',\n",
    "                                      max_depth=15,\n",
    "                                      min_samples_split=12,\n",
    "                                      min_samples_leaf=5,\n",
    "                                      n_jobs = 8,\n",
    "                                      bootstrap=True, \n",
    "                                      random_state=0)\n",
    "linReg = LinearRegression()\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree= 0.7,\n",
    " gamma= 0.0,\n",
    " learning_rate= 0.15,\n",
    " max_depth= 6,\n",
    " min_child_weight= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA ROWS: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NA ROWS: {}\".format(df[all_vars].isnull().any(axis=1).sum()))\n",
    "regression_data = df[all_vars]\n",
    "# Write out for any additional EDA\n",
    "regression_data.to_csv(YEAR+\"_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Send over final data to R\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "    ro.globalenv['regression_data'] = ro.conversion.py2rpy(regression_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_vars = ['AllServices_Binary',\"YoungerServices_Binary\", \"FinancialServices_Binary\"]\n",
    "\n",
    "feature_sets = {\n",
    "    \"all_features\" : discrete_features+log_continuous_features+binary_features+previous_year_service_features+continuous_features,\n",
    "    \"no_previous_year\": discrete_features+log_continuous_features+binary_features+continuous_features,\n",
    "}\n",
    "\n",
    "# Create folds\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=30)\n",
    "\n",
    "\n",
    "\n",
    "result_dfs = []\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(regression_data):\n",
    "\n",
    "    fold += 1\n",
    "    print(fold)\n",
    "    \n",
    "    for outcome_var in outcome_vars:\n",
    "        fold_results = {}\n",
    "        \n",
    "        print(\"\\t\"+outcome_var)\n",
    "        \n",
    "        y_train = regression_data[outcome_var].iloc[train_index]\n",
    "        y_test = regression_data[outcome_var].iloc[test_index]\n",
    "        \n",
    "        fold_results[\"ind\"] = test_index\n",
    "        fold_results[\"fold\"] = [fold]*len(test_index)\n",
    "        fold_results[\"outcome\"] = [outcome_var]*len(test_index)\n",
    "        fold_results['pred_constant'] = [y_train.mean()] * len(y_test)\n",
    "        \n",
    "        # set up basic reg models (just duplicate across feature sets for simplicity)\n",
    "        basic_model_partial = partial(basic_groupby_model, \n",
    "                                      data = regression_data,\n",
    "                                      outcome_var=outcome_var,\n",
    "                                      train_index=train_index,\n",
    "                                      test_index=test_index)\n",
    "        \n",
    "        fold_results['pred_age'] = basic_model_partial(['age'])\n",
    "        fold_results['pred_age_state'] = basic_model_partial(['age','St'])\n",
    "        fold_results['pred_age_race'] = basic_model_partial(['age','RaceEthn'])\n",
    "        fold_results['pred_age_race_ru13'] = basic_model_partial(['age','RaceEthn','RU13'])\n",
    "        \n",
    "        fold_results['pred_previous_year'] = regression_data.iloc[test_index]['prev_'+outcome_var].values.tolist()\n",
    "        \n",
    "        for feature_set_name, feature_set in feature_sets.items():\n",
    "            \n",
    "            print(\"\\t\\t\"+ feature_set_name)\n",
    "            \n",
    "            onehotencoded_vars = pd.concat([pd.get_dummies(regression_data[var_name],\n",
    "                                                           prefix=var_name)    \n",
    "                                             for var_name in feature_set if var_name in discrete_features],\n",
    "                                           axis=1\n",
    "                                          )\n",
    "            X_matrix = pd.concat([onehotencoded_vars, \n",
    "                                 regression_data[[x for x in feature_set if x not in discrete_features]]],\n",
    "                                axis=1) \n",
    "            \n",
    "            X_train = X_matrix.iloc[train_index]\n",
    "            X_test = X_matrix.iloc[test_index]\n",
    "            reg_call = create_r_formula_string(features=feature_set,\n",
    "                        outcome=outcome_var,\n",
    "                        factor_phrase=\"i\")\n",
    "            \n",
    "            print(\"xgb\")\n",
    "            fold_results['pred_xgb_'+feature_set_name] = xgb_model.fit(X_train,y_train).predict(X_test)\n",
    "            \n",
    "            print(\"negbin\")\n",
    "            fold_results['pred_negbin_'+feature_set_name] = r_negbin_regression_call(reg_call,\n",
    "                                                      ro.IntVector(train_index), \n",
    "                                                      ro.IntVector(test_index))\n",
    "\n",
    "            fold_results['pred_linreg_'+feature_set_name] = linReg.fit(X_train,y_train).predict(X_test)\n",
    "        fold_df = pd.DataFrame(fold_results)\n",
    "        fold_df.to_csv(\"fold_{}.csv\".format(fold))\n",
    "        result_dfs.append(fold_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions\n",
    "results_data = pd.concat(result_dfs,axis=0)\n",
    "regression_data = regression_data.assign(ind= range(len(regression_data)))\n",
    "\n",
    "merged_with_predictions = pd.merge(regression_data,results_data,on='ind')\n",
    "len(merged_with_predictions)\n",
    "\n",
    "len(results_data), len(regression_data)\n",
    "\n",
    "merged_with_predictions.to_csv(\"preds_2018_g.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = X_train\n",
    "shap_values = explainer.shap_values(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv = pd.DataFrame(shap_values)\n",
    "sv.columns=[\"shap_\"+ x for x in X_train.columns]\n",
    "tr = pd.concat((X_train.reset_index(drop=True),sv), axis=1)\n",
    "tr.to_csv(\"df_with_shap.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
